# Default values for idxworker.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  solrPort: &global-solr-port 8983

  ## @param global.storageClass default name of the storageClass to use for PVs
  ## Comment out to use default, if one is set on your cluster
  ##
  ## To inspect your cluster to see what storageClass names are supported:
  ##    $  kubectl get storageclass
  ## (e.g. for Rancher Desktop, use:   storageClass: local-path)
  storageClass: csi-rbd-sc

image:
  repository: ghcr.io/dataoneorg/dataone-index-worker
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  #tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext:
  fsGroup: 1001

securityContext:
  runAsNonRoot: true

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

## @param replicaCount Number of desired index worker pods. NOTE ignored if autoscaling.enabled=true
#replicaCount: 1

nodeSelector: {}

tolerations: []

affinity: {}

persistence:
  claimName: d1index-metacat-pvc
  volumeName: d1index-metacat-pv
  mountPath: /var/metacat
  ## Optional storage class that allows ephemeral volumes to get cleaned up automatically
  ephemeralVolumeStorageClass: csi-cephfs-sc

# Values for the IndexWorker
idxworker:
  ## @param idxworker.mn_url URL of the metacat instance that depends upon this indexer
  ##
  mn_url: "https://valley.duckdns.org/metacat/d1/mn"

  ## @param idxworker.cn_url URL of the CN
  ##
  cn_url: "https://cn.dataone.org/cn"

  ## Location of docs/data within the metacat shared volume
  data_directory: /var/metacat/data
  document_directory: /var/metacat/documents
  # The size of the thread pool which processes the index tasks
  pool_size: 5
  debug: "FALSE"
  d1_serviceType_url: https://cn.dataone.org/mnServiceTypes.xml
  tripleDbDirectory: /etc/dataone/tdb-cache

# The section for the rabbitMQ configuration
rabbitmq:
  enabled: true
  auth:
    username: rmq
    existingPasswordSecret: ""      ## (must contain key: `rabbitmq-password`)

  hostname: d1index-rabbitmq-headless.d1index.svc
  hostport: 5672
  persistence:
    size: 10Gi
  #replicaCount: 3
  #If you change the number of the max priority, the existing queue must be deleted, and consumers
  # must use the same number.
  #max.priority: 10

solr:
  collection: temp_collection
  customCollection: dataone_index
  coreNames:
    - dataone_core
  #javaMem: "-Xms512m -Xmx2g"
  hostname: d1index-solr-headless.d1index.svc
  service:
    ports:
      http: *global-solr-port
    nodePorts:
      http: *global-solr-port
  containerPorts:
    http: *global-solr-port
  auth:
    ## Nov 2023: Agreed with Matt we don't need solr auth for now, since solr isn't exposed
    ## outside the cluster. Can add later if needed by anyone not using the subchart, and
    ## instead connecting to a solr instance outside the cluster
    ##
    enabled: false
  persistence:
      size: 10Gi
  extraVolumes:
    - name: solr-config
      configMap:
        name: d1index-configfiles
        defaultMode: 0777
  extraVolumeMounts:
    - name: solr-config
      mountPath: /solrconfig
    - name: solr-config
      mountPath: /opt/bitnami/scripts/solr/entrypoint.sh
      subPath: entrypoint.sh
  lifecycleHooks:
    postStart:
      exec:
        command: ["/bin/bash", "-c", "/solrconfig/config-solr.sh"]
  containerSecurityContext:
    runAsUser: 1000
